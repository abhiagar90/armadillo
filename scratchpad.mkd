## Introduction#

By common opinion, a democratic society in modern world is a misnomer - an illusion often given to the "ruled" party in a nation. Two centuries after Lincoln's definition of democracy, a government-for-the-people sadly is nothing but a collection of handful of population in control of basic resources of a country (natural or artificial). Leaders in military, politics, businesses, sports and arts thus become the actual voice of a country rather than the common mass. The distribution of power in these domains are often hereditary (among close ties in family) instead of elected representatives. Interactions among the persons of these important fields are also common for their effective functioning ( or often to safeguard their self benefits.)

In this regard, accountability and transparency in government is one of the key requirements in order to obtain an ideal democratic society. Unfortunately the lack of proper knowledge about the politicians and corporates has led to new forms of "collective" dictatorship where public rights or voices are rendered ineffective.

To account for this growing problem of opacity we have tried to study the social network of Indian politicians and corporates and disemminate the information to the common public.
##Problem Statement#
Problem of forming the social networks of Indian Politicians and Corporates, visualizing  and analysing them.

## Objectives#
We intended to complete following tasks through our project-
    - To collect data from semantic web (and other sources) to form a database ( henceforth referred to as (b) "knowledge base" (b) / (b) "knowledge graph" (b) )
    - To create a neat, structured minimal error data collection which otherwise is scattered at respective sources.
    - To provide a data mashup from different fields to further help the academecians, journalists etc.
    - For monitoring the top players in Indian society - mainly in the spheres of politics and businesses in India.
    - To disemminate information to public  which brings about accountability and transparency. 
    -  To seek answers to questions like -
         who were the big players in Indian politics and businesses?
         Is there any influence (or possibility of it) of political field by a person in corporate field?
         How important is one politician in a network of politicians (or a businessperson in a business network)?
         Whom does actual power reside in a democracy.
We believe that through our work, we will be able to show how such system of inter-disciplinary data helps to spread information and find patterns and discover more knowledge.

#Thesis organization#
The rest of the thesis has been divided into following -
    1. Social Network Creation.
    2. Design of Power Elites Web App. 
    3. Pattern Analysis

## Related work#
        Previously interactions between top players of the society has been studied by C.W. Mills in his book "The Power Elites" where he mentions about the interwoven interests of leaders in military, political, corporate, educational level. Furthermore, Thomas Piketty in his book "Capital in Twenty First Century" discusses the new age of capitalism with data collected from journalists in Europe. Piketty believes in lines with C.W. Mills that the power in a so-called democratic society truly resides on a handful of individuals holding important resources. The above works are mainly concentrated in the background of Europe and US but is equally important and viable in developing countries like India as well. In justification, Patrick French has analysed the Indian politics revealing the hereditary nature of Indian political relationships in his book - "India: A Portrait". A recent work titled "Who owns your media" showed the relations or influence of big business families over media ownerships shows similar interesting patterns where public resources are controlled by handful of corporates.
        Technically a knowledge base is a data store to keep unstructured information for using it to analyse patterns, create new inferences or just to diseminate information. Works on creating similar things has been done by makers of Littlesis.org, poderopedia.org etc. LittleSis.org is an US based website reflecting eminent personalities in the govt. of United States. The website shows networks of US politicians, member of govts, businessmen etc through visualizations and/or normal tabular representations. It also has a wiki system where any registered user can modify the content (basically crowd source the content to add more information). Some analysis has also been given forward through the use of blogs or articles published in the site. Poderopedia has similar content but for the country of Chile.
## What different and /or better things you have done?
Our work is similar to above in the context of data delivery or information visualization with respect to Indian corporate-political data. We put forward the techniques and challenges encountered to build a knowledge base and its application to find patterns or new knowledge.

## Things to be added in background work #
The core two things that required our special attention when working with several data sources is the process of modelling the database as a graph and method of resolving same entities from different data sources. These two things are problems with extensive study of their own.

Entity resolution has been studied since 1946 by works of Halber.L.Dunn[citation]. Basic method is to match a pair of strings accurately to determine possible similar entities. Since then, several string matching algorithms has been used for this purpose. The levenshtein algorithm [citations] gives score based on no of edits to convert one string to another.It is escpecially useful for to deal with problem of mis-spelt records. Improving on that the Jaro-Winkler algo [citation] looks at matching characters within a small range while giving scores. This is suitable for short strings such as names and fits our purpose well. Another class of string matching algorithms looks at phonetics to resolve entities with similar sounding text. The soundex algorithm[citation] is one of the most well known.These algorithms use english language pronunciations to create an index of the text. For our purpose, we have used the Jaro-Winkler and a variation of Soundex (Double Metaphone)[citations]

    The graph model of the data is required since we are emphasizing the relationships among the data. Graph databases as a concept existed long since mid-1960s when IBM IMS [citation]supported graph structures in its hierarchical model. Graph databases allows one to give semantic queries  over data relationships. A graph databases model the data as nodes and relationships among them as edges between nodes. Internally, they store the data as a relational table (MariaDB[citation]) or through document-value stores (Neo4j, OrientDB[citation]). For the purpose of our project we have used the Neo4j database to store the data. 

##TODO - Softare components 
    #TODO - Neo4j
        [pic]
        #TODO - What is Neo4j?
            Neo4j is a commercial graph database from Neo Technologies. It stores the data in form of nodes (table entries in relational database) edges (relationships, which were stored as data in relational database and fetched by JOINs) , and labels (to name the data points similar to table names). 
        #TODO - Why is it required? (Graph db advantages here - cypher queries)
        - One advantage of Neo4j is it being NoSQL database does not use or have any specific data schema. Which means new properties can be added to it anytime.
        - With the help of explicitly stored relations, Neo4j allows us to make queries like -
            * Find all companies directly, Naveen Jindal coming under the influence of Naveen Jindal ? 
            * Find the entity with maximum degree?

        #TODO - How is it used here? (Core and Crawl DB)
        - Crawl DB - A database to store the crawled data.
        - Core DB - The heart of the system which stores the knowledge base.

    #TODO - Solr 
        [pic]
        #TODO - What is Solr?
        #TODO - Why required here? (Indexing advantages)
        #TODO - How is it used here? (In resolving entities by searching db)
                (Also how is it getting updated)

    #TODO - Flask Web Framework
        [pic]
        #TODO - Why used? (Of all other frameworks why used)
                (Non availability of neo4j ORMs)
        #TODO - How is it used? (How website front end backend glues etc.)
        
    #MySQL DBs 
        [pic]
        #TODO - Why used? Why not graphDB again or any other replacement?
        #TODO - How is it used? (META TABLE AND INDEXING)


##TODO - Entity Resolution

    #TODO - Entity resolution basics
        #TODO basic pseudocode of the algo and its explanations
        The basic algorithm of entity resolution goes as follows-

        Let us assume we have two datasets S and T
        Datasets - S, T
        Results - list -L consisting of proper matches of T in S
        Initializations - L is empty
        Parameters - S,T, threshold
        for all records in t in T:
            search for t in S
            pick records s in S for which match-score(t, s) >= threshold
            append s in L

        Complexity of above algorithm is O(n*m) where n is size(S) and m is size(T) when n records are searched linearly in S.
        From the above, we can find that the two crucial operations here is the choice of proper scoring algorithm to pick s and search of t in S in line [line].
    #TODO - String Matching Algorithm
        The requirement of a scoring algorithm was such that it should match words with small spelling mistakes and similar names. In our case, we have used Jaro distance as it considers ranges and transpositions while matching two words. A sample experiment from our side on a list of four names showed the following scores - 
        [[tble]]
        Upon a test run of all algorithms over a sample of 100 names from our datasets we decided to use jaro-winkler for the purpose. We also required to match names which are phonetically similar to other names. These helped us to match names like 'Gautam' and 'Gautambhai', 'Vidya', 'Bidya' and 'Viday' etc.

    #TODO - Comparison Techniques
        As obvious from above algorithm the performance of entity matching algorithm depends largely on how fast the searching occurs in dataset S. A naive algorithm like above added a factor of O(n) on linear search over entire S. The obvious improvement over it is the possible implementation of a binary search to reduce search speed to O(log n) time. But binary search is not applicable here for following two reasons -
            - Binary search uses a predicate like gt, eq, lt , the value of which is true or false. Such predicates determines some order in the dataset. 
            - The absence of such predicates as above avoid any sorting or ranking of the data in any order.
        As a result, the performance of the entire system got bottlenecked by the resolution module. As a result we sought out for other solutions regarding this.

    #TODO - Machine Learning techniques
        After having performance bottleneck in searching records in other datasets, we looked for probabilistic ways of solving the same problem. We used the python library of csvdedupe for this purpose. The library basically induced an active learning mechanism to obtain training samples where it picked two entities of possible match and prompts the user to label positive/negative. It then matches the related entities accordingly with the hypothesis formed. Unfortunately this approach suffered from following drawbacks-
        - Too small data to accurately form a proper hypothesis. 
        - The labels that were asked to mark alongwith data were picked at random and often the results of the algorithm are different in different runs (depending on the number of positive or negative label given at that time).

        Fortunately indexing other dataset helped to come out of this.

    #TODO - Indexing and Apache Solr
        Indexing allows searching to be very fast of near O(1) speed. Indexes are datastructures that store contents of a document (in our context fields of records in a dataset) for faster access to the document. This enables faster retrieval with a trade off for using more memory. For our purpose we used Apache Solr framework for the searching step.Solr uses Apache Lucene to create an inverted index based desired on fields in the records. An inverted index basically creates a data structure on the content of the records and have pointers to the actual locations of the records. So for all text in the specified fields, Lucene breaks them (tokenizes them) and store them in a data structure for fast retrieval.  Solr also sets up a  Web server with REST APIs to allow us to integrate it with other parts of our system described in chapter [give reference here]. 
        Since Solr has its own sets of protocols we had to modify the way we apply the resolution algorithm described above. The main steps followed by us to realize this are as follows. 

        - Defining a data set to index  (the data set S above). We described a data source which contained the dataset. In this case, it was a mysql database. (file db-data-config.xml) .
        - Defining the fields to index. Solr needs to have a schema of the records to know which fields will indexed and which one is kept as satellite data. These are specified in solr configuration files. (schema.xml)
        - Defining how to preprocess all the terms before creating the index. Solr allows to specify a list of pre-built tokenizers, stemmers, filters to preprocess a term or custom ones if necessary. We used whitespace tokenizers and double metaphone phonetic filters to get a match score relevant to our purpose and in this way used the phonetic algorithm for better text matching. (file - solrconfig.xml)
        - Forming a lucene query based on the contents of records of another dataset (data set T above)
        - Applying Jaro algorithm for comparison was difficult in Solr. This is because, all the functions applied to the text for indexing are necessarily single parameter. Jaro or any other non-phonetic string metric needs at lest two string Results returned by the Solr is further filtered by the Jaro-Winkler algorithm. Results being quite small, does not take much time even if a one-one matching algo is executed. 

    #Solr Fields 
    To effectively triangulate two entities, special emphasis was given on which fields to compare while doing it. After few experiments we decided to match records on following fields generated from the graph data model we discussed in previous section.

    - Aliases - a list that contains names pertaining to an entity. This is especially required when a person/institution is known by several names in the world. Eg. - Narendra Modi vs Narendra Damodar Modi, BJP vs Bharatiya Janata Party 
    - Aliases Phonetic - same as above but here search is done on phonetic index.
    - label - Label dictates the type of entity as per data model in chapter [refer chapter here].
    - Keywords - a list that contains main keywords of the entity. This field is most helpful in triangulation as it indexes all the properties of the entity and the aliases of the entities directly related with the original. Important properties unique to a particular entity like location for a particular item.

    #Lucene Queries

    Proper search queries are essential to effeciently resolve an entity. These involved using above fields effectively to obtain relevant entities- 
    The grammar of the lucene query used by us goes as follows-

    L:= I:(Q) AND L | epsilon

    where I is the index field (one of above) and Q is the query string to be matched against I
    Where index field is one of the above and query string can be as follows -
    "string text" - matching the exact word 'string text'
    string text - matches string or text or both
    string~ - lucene applies edit distance to string and returns the possible matches

    The query we used for our purpose is as follows-
    " labels:(each label separated by whitespace) aliases:(each alias~ separated by whitespace ) AND aliases_phonetic:(each alias separated by whitepace) AND keywords:( each keyword~ separated by whitespace)"

    Eg. - 

    #Performance -

    On testing our Solr integrated system with the original basic resolution algorithm, we found many fold improvements. Initially upon resolving 1000 corporate entities against about 500 political ones over the "name" field in both took us about 150 minutes to resolve. That makes resolution of a single entity about 18s.
    Compared to that, a single entity is resolved in Solr in little over 1s showing a performance upgrade of about 18x.

